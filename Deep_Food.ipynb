{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "tf.__version__ #tf version project ran on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -V && nvidia-smi #gpu info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"helper_functions.py\"):\n",
    "    !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
    "else:\n",
    "    print(\"[INFO] 'helper_functions.py' already exists, skipping download.\") #useful script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import create_tensorboard_callback, plot_loss_curves, compare_historys\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# getting dataset\n",
    "(train_data ,test_data), ds_info = tfds.load(name='food101',\n",
    "                                               split=['train', 'validation'],\n",
    "                                               shuffle_files=True,\n",
    "                                               as_supervised=True,\n",
    "                                               with_info=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset info\n",
    "ds_info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class names\n",
    "class_names = ds_info.features[\"label\"].names\n",
    "class_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_data.take(1)\n",
    "sample\n",
    "# image info\n",
    "for image, label in sample:\n",
    "  print(f\"\"\"\n",
    "  Image shape: {image.shape}\n",
    "  Image dtype: {image.dtype}\n",
    "  Target class from Food101 (tensor form): {label}\n",
    "  Class name (str form): {class_names[label.numpy()]}\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image)\n",
    "plt.title(class_names[label.numpy()]) # add title to image by indexing on class_names list\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "\n",
    "def preprocess_img(image, label, img_shape=224):\n",
    "    image = tf.image.resize(image,[img_shape,img_shape])\n",
    "    return tf.cast(image,tf.float32),label\n",
    "\n",
    "# Preprocess a single sample image and check the outputs\n",
    "preprocessed_img = preprocess_img(image, label)[0]\n",
    "plt.imshow(preprocessed_img/255.)\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Map preprocessing function to training data (and paralellize)\n",
    "train_data = train_data.map(map_func=preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# Shuffle train_data and turn it into batches and prefetch it (load it faster)\n",
    "train_data = train_data.shuffle(buffer_size=1000).batch(batch_size=32).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Map prepreprocessing function to test data\n",
    "test_data = test_data.map(preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# Turn test data into batches (don't need to shuffle)\n",
    "test_data = test_data.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import create_tensorboard_callback\n",
    "\n",
    "# Create ModelCheckpoint callback to save model's progress\n",
    "checkpoint_path = \"model_checkpoints/cp.ckpt\" # saving weights requires \".ckpt\" extension\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                      monitor=\"val_accuracy\", # save the model weights with best validation accuracy\n",
    "                                                      save_best_only=True, # only save the best weights\n",
    "                                                      save_weights_only=True, # only save model weights (not whole model)\n",
    "                                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy(policy=\"mixed_float16\")\n",
    "mixed_precision.global_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "INPUT_SHAPE = (224,224,3)\n",
    "base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False)\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = layers.Input(shape=INPUT_SHAPE, name='input_layer')\n",
    "x = base_model(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D(name='pool_layer')(x)\n",
    "x = layers.Dense(len(class_names),name='output_layer')(x)\n",
    "outputs = layers.Activation('softmax', dtype= tf.float32, name='activation_layer')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs,outputs)\n",
    "\n",
    "# compile\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[1].layers[:20]: # only check the first 20 layers to save output space\n",
    "    print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "\n",
    "history = model.fit(train_data,\n",
    "                    epochs=3,\n",
    "                    steps_per_epoch=len(train_data),\n",
    "                    validation_data=test_data,\n",
    "                    validation_steps=int(0.15*len(test_data)),\n",
    "                    callbacks=[create_tensorboard_callback('training_logs','Deep_Food_Model'), \n",
    "                               model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_evaluation = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a function to recreate the original model\n",
    "def create_model():\n",
    "  # Create base model\n",
    "  input_shape = (224, 224, 3)\n",
    "  base_model = tf.keras.applications.efficientnet.EfficientNetB0(include_top=False)\n",
    "  base_model.trainable = False # freeze base model layers\n",
    "\n",
    "  # Create Functional model \n",
    "  inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n",
    "  # Note: EfficientNetBX models have rescaling built-in but if your model didn't you could have a layer like below\n",
    "  # x = layers.Rescaling(1./255)(x)\n",
    "  x = base_model(inputs, training=False) # set base_model to inference mode only\n",
    "  x = layers.GlobalAveragePooling2D(name=\"pooling_layer\")(x)\n",
    "  x = layers.Dense(len(class_names))(x) # want one output neuron per class \n",
    "  # Separate activation of output layer so we can output float32 activations\n",
    "  outputs = layers.Activation(\"softmax\", dtype=tf.float32, name=\"softmax_float32\")(x) \n",
    "  model = tf.keras.Model(inputs, outputs)\n",
    "  \n",
    "  return model\n",
    "\n",
    "# 2. Create and compile a new version of the original model (new weights)\n",
    "created_model = create_model()\n",
    "created_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                      optimizer=tf.keras.optimizers.Adam(),\n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "# 3. Load the saved weights\n",
    "created_model.load_weights(checkpoint_path)\n",
    "\n",
    "# 4. Evaluate the model with loaded weights\n",
    "results_created_model_with_loaded_weights = created_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in created_model.layers[1].layers[:20]: # check only the first 20 layers to save printing space\n",
    "    print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"07_efficientnetb0_feature_extract_model_mixed_precision\"\n",
    "model.save(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_saved_model = tf.keras.models.load_model(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the layers in the base model and see what dtype policy they're using\n",
    "for layer in loaded_saved_model.layers[1].layers[:20]: # check only the first 20 layers to save output space\n",
    "    print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_loaded_saved_model = loaded_saved_model.evaluate(test_data)\n",
    "results_loaded_saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "assert np.isclose(base_evaluation, results_loaded_saved_model).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = tf.keras.models.load_model(\"/07_efficientnetb0_feature_extract_model_mixed_precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a summary of our downloaded model\n",
    "fine_tuned_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_loaded_gs_model = fine_tuned_model.evaluate(test_data)\n",
    "results_loaded_gs_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in fine_tuned_model.layers:\n",
    "    layer.trainable = True # set all layers to trainable\n",
    "    print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in fine_tuned_model.layers[1].layers[:20]:\n",
    "    print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", # watch the val loss metric\n",
    "                                                  patience=3) # if val loss decreases for 3 epochs in a row, stop training\n",
    "\n",
    "# Create ModelCheckpoint callback to save best model during fine-tuning\n",
    "checkpoint_path = \"fine_tune_checkpoints/\"\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                      save_best_only=True,\n",
    "                                                      monitor=\"val_loss\")\n",
    "\n",
    "# Creating learning rate reduction callback\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",  \n",
    "                                                 factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x)\n",
    "                                                 patience=2,\n",
    "                                                 verbose=1, # print out when learning rate goes down \n",
    "                                                 min_lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model.compile(loss=\"sparse_categorical_crossentropy\", # sparse_categorical_crossentropy for labels that are *not* one-hot\n",
    "                        optimizer=tf.keras.optimizers.Adam(0.0001), # 10x lower learning rate than the default\n",
    "                        metrics=[\"accuracy\"])\n",
    "\n",
    "fine_tuned_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_history = fine_tuned_model.fit(train_data,\n",
    "                                                        epochs=100, # fine-tune for a maximum of 100 epochs\n",
    "                                                        steps_per_epoch=len(train_data),\n",
    "                                                        validation_data=test_data,\n",
    "                                                        validation_steps=int(0.15 * len(test_data)), # validation during training on 15% of test data\n",
    "                                                        callbacks=[create_tensorboard_callback(\"training_logs\", \"efficientb0_101_classes_all_data_fine_tuning\"), # track the model training logs\n",
    "                                                                   model_checkpoint, # save only the best model during training\n",
    "                                                                   early_stopping, # stop model after X epochs of no improvements\n",
    "                                                                   reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model.save(\"07_efficientnetb0_fine_tuned_101_classes_mixed_precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(fine_tuned_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(fine_tuned_history)['lr'].plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
